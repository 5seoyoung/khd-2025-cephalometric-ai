# -*- coding: utf-8 -*-
"""
Multimodal Classifier
ì„ìƒ ì§€í‘œì™€ ë©”íƒ€ë°ì´í„°ë¥¼ ê²°í•©í•˜ì—¬ ê³¨ê²©ì„± ë¶€ì •êµí•©ì„ ë¶„ë¥˜í•©ë‹ˆë‹¤.

ì£¼ìš” ê¸°ëŠ¥:
- Rule-based ë¶„ë¥˜ (ANB ê¸°ì¤€)
- Seeded XGBoost ì‹œë®¬ë ˆì´ì…˜ 
- ì‹ ë¢°ë„ ê³„ì‚° (ê²°ì •ê²½ê³„ì™€ì˜ ê±°ë¦¬ ê¸°ë°˜)
- ë¶„ë¥˜ ê·¼ê±° ì œê³µ
"""

import numpy as np
import json
from typing import Dict, Any, Tuple, Optional
import math

# ë¶€ì •êµí•© ë¶„ë¥˜ ìƒìˆ˜
CLASS_LABELS = {
    0: "Class I",
    1: "Class II", 
    2: "Class III"
}

CLASS_DESCRIPTIONS = {
    0: "ê³¨ê²©ì ìœ¼ë¡œ ì •ìƒ",
    1: "ê³¨ê²©ì ìœ¼ë¡œ ìƒì•… ê³¼ì„±ì¥ ë˜ëŠ” í•˜ì•… ì—´ì„±ì¥",
    2: "ê³¨ê²©ì ìœ¼ë¡œ í•˜ì•… ê³¼ì„±ì¥ ë˜ëŠ” ìƒì•… ì—´ì„±ì¥"
}

# ANB ê¸°ì¤€ ì„ê³„ê°’
ANB_THRESHOLDS = {
    "class_1_range": [0, 4],      # Class I: 0Â° â‰¤ ANB â‰¤ 4Â°
    "class_2_threshold": 4,       # Class II: ANB > 4Â°
    "class_3_threshold": 0        # Class III: ANB < 0Â°
}

def rule_based_classification(anb_value: float) -> int:
    """
    ANB ê°ë„ ê¸°ë°˜ ê·œì¹™ ë¶„ë¥˜
    
    Args:
        anb_value: ANB ê°ë„ (degrees)
    
    Returns:
        í´ë˜ìŠ¤ ë²ˆí˜¸ (0: Class I, 1: Class II, 2: Class III)
    """
    if anb_value > ANB_THRESHOLDS["class_2_threshold"]:
        return 1  # Class II
    elif anb_value < ANB_THRESHOLDS["class_3_threshold"]:
        return 2  # Class III
    else:
        return 0  # Class I

def calculate_confidence_from_distance(anb_value: float, 
                                     predicted_class: int, 
                                     sigma: float = 2.0) -> float:
    """
    ê²°ì •ê²½ê³„ë¡œë¶€í„°ì˜ ê±°ë¦¬ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ì‹ ë¢°ë„ë¥¼ ê³„ì‚°í•©ë‹ˆë‹¤.
    
    Args:
        anb_value: ANB ê°ë„
        predicted_class: ì˜ˆì¸¡ëœ í´ë˜ìŠ¤
        sigma: ì‹ ë¢°ë„ ê³„ì‚°ìš© ìŠ¤ì¼€ì¼ë§ íŒŒë¼ë¯¸í„°
    
    Returns:
        ì‹ ë¢°ë„ (0.0 ~ 1.0)
    """
    if predicted_class == 1:  # Class II
        # ANB > 4ì¸ ê²½ìš°, 4ë¡œë¶€í„°ì˜ ê±°ë¦¬
        distance = max(0, anb_value - ANB_THRESHOLDS["class_2_threshold"])
    elif predicted_class == 2:  # Class III  
        # ANB < 0ì¸ ê²½ìš°, 0ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬
        distance = max(0, ANB_THRESHOLDS["class_3_threshold"] - anb_value)
    else:  # Class I
        # 0~4 ë²”ìœ„ ë‚´ì—ì„œ ì¤‘ì‹¬(2)ìœ¼ë¡œë¶€í„°ì˜ ê±°ë¦¬
        center = 2.0
        distance_from_center = abs(anb_value - center)
        # Class Iì˜ ê²½ìš° ì¤‘ì‹¬ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ë†’ì€ ì‹ ë¢°ë„
        return 1.0 - min(1.0, distance_from_center / 2.0)
    
    # ì‹œê·¸ëª¨ì´ë“œ í•¨ìˆ˜ë¡œ ì‹ ë¢°ë„ ê³„ì‚°
    confidence = 1.0 - math.exp(-distance / sigma)
    return min(0.95, max(0.5, confidence))  # 0.5~0.95 ë²”ìœ„ë¡œ ì œí•œ

def seeded_xgboost_simulation(features: Dict[str, Any], seed: int = 42) -> np.ndarray:
    """
    XGBoost ì‹œë®¬ë ˆì´ì…˜ (ê²°ì •ë¡ ì  ë‚œìˆ˜ ê¸°ë°˜)
    
    ì‹¤ì œ XGBoost ëŒ€ì‹  íŠ¹ì„±ê°’ ê¸°ë°˜ í•´ì‹œì™€ ì‹œë“œë¥¼ ì¡°í•©í•˜ì—¬
    ì¼ê´€ëœ í™•ë¥  ë¶„í¬ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
    
    Args:
        features: ì…ë ¥ íŠ¹ì„±ë“¤
        seed: ë‚œìˆ˜ ì‹œë“œ
    
    Returns:
        3ê°œ í´ë˜ìŠ¤ì— ëŒ€í•œ í™•ë¥  ë¶„í¬
    """
    # íŠ¹ì„±ê°’ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜í•˜ì—¬ í•´ì‹œ ìƒì„±
    feature_str = ""
    for key in sorted(features.keys()):
        if isinstance(features[key], (int, float)):
            feature_str += f"{key}:{features[key]:.2f};"
        else:
            feature_str += f"{key}:{features[key]};"
    
    # íŠ¹ì„± ê¸°ë°˜ í•´ì‹œë¥¼ ì‹œë“œì™€ ì¡°í•©
    feature_hash = abs(hash(feature_str)) % 10000
    combined_seed = (seed + feature_hash) % (2**31)
    
    # ì‹œë“œ ê³ ì • ë‚œìˆ˜ ìƒì„±ê¸°
    rng = np.random.RandomState(combined_seed)
    
    # 3ê°œ í´ë˜ìŠ¤ì— ëŒ€í•œ ë¡œì§“ ìƒì„±
    logits = rng.normal(0, 1, 3)
    
    # ANB ê°’ì— ë”°ë¥¸ ë°”ì´ì–´ìŠ¤ ì¶”ê°€ (ì•½ê°„ì˜ ë„ë©”ì¸ ì§€ì‹ ë°˜ì˜)
    anb = features.get("ANB", 2.0)
    if anb > 4:
        logits[1] += 0.5  # Class II ì„ í˜¸
    elif anb < 0:
        logits[2] += 0.5  # Class III ì„ í˜¸
    else:
        logits[0] += 0.3  # Class I ì„ í˜¸
    
    # ì†Œí”„íŠ¸ë§¥ìŠ¤ë¡œ í™•ë¥  ë³€í™˜
    exp_logits = np.exp(logits - np.max(logits))  # ìˆ˜ì¹˜ ì•ˆì •ì„±
    probabilities = exp_logits / np.sum(exp_logits)
    
    return probabilities

def extract_features_from_metrics(metrics: Dict[str, Any], 
                                meta: Dict[str, Any]) -> Dict[str, Any]:
    """
    ì„ìƒ ì§€í‘œì™€ ë©”íƒ€ë°ì´í„°ë¡œë¶€í„° ë¶„ë¥˜ìš© íŠ¹ì„±ì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
    """
    features = {}
    
    # ì„ìƒ ì§€í‘œ íŠ¹ì„±
    for metric_name in ["SNA", "SNB", "ANB", "FMA"]:
        if metric_name in metrics:
            features[metric_name] = metrics[metric_name]["value"]
            features[f"{metric_name}_status"] = metrics[metric_name]["status"]
    
    # ë©”íƒ€ë°ì´í„° íŠ¹ì„±
    features["age"] = meta.get("age", 25)
    features["sex"] = meta.get("sex", "U")
    
    # íŒŒìƒ íŠ¹ì„±
    sna = features.get("SNA", 82)
    snb = features.get("SNB", 80)
    features["SNA_SNB_diff"] = sna - snb  # ANBì™€ ë™ì¼í•˜ì§€ë§Œ ë…ë¦½ ê³„ì‚°
    
    # ì •ìƒ ë²”ìœ„ ì´íƒˆ ì •ë„
    anb = features.get("ANB", 2)
    if anb > 4:
        features["ANB_deviation"] = anb - 4
    elif anb < 0:
        features["ANB_deviation"] = abs(anb)
    else:
        features["ANB_deviation"] = 0
    
    return features

class DemoClassifier:
    """
    ë°ëª¨ìš© ë©€í‹°ëª¨ë‹¬ ë¶„ë¥˜ê¸°
    """
    
    def __init__(self, seed: int = 42, rule_weight: float = 0.7):
        """
        ì´ˆê¸°í™”
        
        Args:
            seed: ë‚œìˆ˜ ì‹œë“œ
            rule_weight: ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜ì˜ ê°€ì¤‘ì¹˜ (0~1)
        """
        self.seed = seed
        self.rule_weight = rule_weight
        self.model_weight = 1.0 - rule_weight
        
        print(f"âœ… DemoClassifier ì´ˆê¸°í™” ì™„ë£Œ (seed={seed}, rule_weight={rule_weight})")
    
    def predict(self, 
                metrics: Dict[str, Any], 
                meta: Dict[str, Any] = None) -> Dict[str, Any]:
        """
        ë¶€ì •êµí•© ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        
        Args:
            metrics: clinical_metrics.compute_all()ì˜ ê²°ê³¼
            meta: ë©”íƒ€ë°ì´í„° (ë‚˜ì´, ì„±ë³„ ë“±)
        
        Returns:
            ë¶„ë¥˜ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        if meta is None:
            meta = {}
        
        # ANB ê°’ ì¶”ì¶œ
        anb_value = metrics["ANB"]["value"]
        
        # 1. ê·œì¹™ ê¸°ë°˜ ë¶„ë¥˜
        rule_class = rule_based_classification(anb_value)
        rule_confidence = calculate_confidence_from_distance(anb_value, rule_class)
        
        # 2. íŠ¹ì„± ì¶”ì¶œ
        features = extract_features_from_metrics(metrics, meta)
        
        # 3. ì‹œë®¬ë ˆì´ì…˜ ëª¨ë¸ ì˜ˆì¸¡
        model_probs = seeded_xgboost_simulation(features, self.seed)
        model_class = int(np.argmax(model_probs))
        model_confidence = float(np.max(model_probs))
        
        # 4. ê°€ì¤‘ í‰ê· ìœ¼ë¡œ ìµœì¢… í™•ë¥  ê³„ì‚°
        final_probs = np.zeros(3)
        final_probs[rule_class] += self.rule_weight
        final_probs += self.model_weight * model_probs
        
        # ì •ê·œí™”
        final_probs = final_probs / np.sum(final_probs)
        
        # 5. ìµœì¢… ì˜ˆì¸¡
        final_class = int(np.argmax(final_probs))
        final_confidence = float(np.max(final_probs))
        
        # 6. ê²°ê³¼ êµ¬ì„±
        result = {
            "predicted_class": final_class,
            "predicted_label": CLASS_LABELS[final_class],
            "confidence": final_confidence,
            "probabilities": {
                CLASS_LABELS[i]: float(prob) for i, prob in enumerate(final_probs)
            },
            "anb_value": anb_value,
            "classification_basis": self._get_classification_basis(anb_value, final_class),
            "components": {
                "rule_based": {
                    "class": rule_class,
                    "confidence": rule_confidence,
                    "weight": self.rule_weight
                },
                "model_based": {
                    "class": model_class, 
                    "confidence": model_confidence,
                    "weight": self.model_weight
                }
            }
        }
        
        return result
    
    def _get_classification_basis(self, anb_value: float, predicted_class: int) -> str:
        """
        ë¶„ë¥˜ ê·¼ê±°ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        basis_parts = []
        
        # ANB ê¸°ë°˜ ê·¼ê±°
        if predicted_class == 1:  # Class II
            basis_parts.append(f"ANB {anb_value:.1f}Â° > 4Â° (ìƒì•… ê³¼ì„±ì¥ ì§€ì‹œ)")
        elif predicted_class == 2:  # Class III
            basis_parts.append(f"ANB {anb_value:.1f}Â° < 0Â° (í•˜ì•… ê³¼ì„±ì¥ ì§€ì‹œ)")
        else:  # Class I
            basis_parts.append(f"ANB {anb_value:.1f}Â° (ì •ìƒ ë²”ìœ„ 0-4Â°)")
        
        # ì‹ ë¢°ë„ ì •ë³´
        basis_parts.append(f"ê·œì¹™ ê¸°ë°˜ {self.rule_weight*100:.0f}% + ëª¨ë¸ ê¸°ë°˜ {self.model_weight*100:.0f}%")
        
        return " | ".join(basis_parts)
    
    def get_classifier_info(self) -> Dict[str, Any]:
        """
        ë¶„ë¥˜ê¸° ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        return {
            "classifier": "DemoClassifier",
            "version": "1.0",
            "seed": self.seed,
            "rule_weight": self.rule_weight,
            "model_weight": self.model_weight,
            "classes": list(CLASS_LABELS.values()),
            "primary_feature": "ANB angle"
        }

def test_multimodal_classifier():
    """
    ë©€í‹°ëª¨ë‹¬ ë¶„ë¥˜ê¸° í…ŒìŠ¤íŠ¸
    """
    print("ğŸ§ª DemoClassifier í…ŒìŠ¤íŠ¸")
    print("=" * 50)
    
    try:
        # 1. ë¶„ë¥˜ê¸° ì´ˆê¸°í™”
        classifier = DemoClassifier(seed=42, rule_weight=0.7)
        
        # 2. ë¶„ë¥˜ê¸° ì •ë³´ ì¶œë ¥
        info = classifier.get_classifier_info()
        print("ğŸ“‹ ë¶„ë¥˜ê¸° ì •ë³´:")
        for key, value in info.items():
            print(f"   {key}: {value}")
        
        # 3. í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ë“¤
        test_cases = [
            {
                "name": "Class I (ì •ìƒ)",
                "metrics": {
                    "SNA": {"value": 82.5, "status": "normal"},
                    "SNB": {"value": 80.2, "status": "normal"},
                    "ANB": {"value": 2.3, "status": "normal"},
                    "FMA": {"value": 27.8, "status": "normal"}
                },
                "meta": {"age": 25, "sex": "F"}
            },
            {
                "name": "Class II (ìƒì•… ê³¼ì„±ì¥)",
                "metrics": {
                    "SNA": {"value": 85.0, "status": "high"},
                    "SNB": {"value": 78.0, "status": "normal"},
                    "ANB": {"value": 7.0, "status": "high"},
                    "FMA": {"value": 28.0, "status": "normal"}
                },
                "meta": {"age": 30, "sex": "M"}
            },
            {
                "name": "Class III (í•˜ì•… ê³¼ì„±ì¥)",
                "metrics": {
                    "SNA": {"value": 80.0, "status": "normal"},
                    "SNB": {"value": 85.0, "status": "high"},
                    "ANB": {"value": -5.0, "status": "low"},
                    "FMA": {"value": 26.0, "status": "normal"}
                },
                "meta": {"age": 28, "sex": "F"}
            }
        ]
        
        # 4. ê° í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ ì‹¤í–‰
        for i, case in enumerate(test_cases, 1):
            print(f"\nğŸ” í…ŒìŠ¤íŠ¸ ì¼€ì´ìŠ¤ {i}: {case['name']}")
            
            result = classifier.predict(case["metrics"], case["meta"])
            
            print(f"   ì˜ˆì¸¡ ê²°ê³¼: {result['predicted_label']}")
            print(f"   ì‹ ë¢°ë„: {result['confidence']*100:.1f}%")
            print(f"   ANB: {result['anb_value']:.1f}Â°")
            print(f"   ê·¼ê±°: {result['classification_basis']}")
            
            # í™•ë¥  ë¶„í¬ ì¶œë ¥
            print("   í™•ë¥  ë¶„í¬:")
            for label, prob in result["probabilities"].items():
                print(f"     {label}: {prob*100:.1f}%")
        
        # 5. ì¬í˜„ì„± í…ŒìŠ¤íŠ¸
        print("\nğŸ”„ ì¬í˜„ì„± í…ŒìŠ¤íŠ¸:")
        result1 = classifier.predict(test_cases[0]["metrics"], test_cases[0]["meta"])
        result2 = classifier.predict(test_cases[0]["metrics"], test_cases[0]["meta"])
        
        confidence_diff = abs(result1["confidence"] - result2["confidence"])
        print(f"   ì‹ ë¢°ë„ ì°¨ì´: {confidence_diff:.6f} (0ì´ë©´ ì™„ì „ ì¬í˜„)")
        
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_multimodal_classifier()