# -*- coding: utf-8 -*-
"""
Integration Pipeline
ì „ì²´ cephalometric ë¶„ì„ ì›Œí¬í”Œë¡œìš°ë¥¼ í†µí•© ì‹¤í–‰í•©ë‹ˆë‹¤.

ì›Œí¬í”Œë¡œìš°:
1) ì´ë¯¸ì§€ ì…ë ¥ ë° ì „ì²˜ë¦¬
2) ëœë“œë§ˆí¬ ì¶”ë¡  (demo_inference)
3) ì„ìƒ ì§€í‘œ ê³„ì‚° (clinical_metrics)
4) ë¶€ì •êµí•© ë¶„ë¥˜ (multimodal_classifier)
5) ê²°ê³¼ í†µí•© ë° ë°˜í™˜
"""

from __future__ import annotations

import os
import sys
import time
import uuid
import json
import logging
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, Union, Tuple, List

from PIL import Image

__all__ = ["CephalometricPipeline"]

# --------------------------------------------------------------------------------------
# ë¡œê¹… ì„¤ì • (Streamlit/CLI ëª¨ë‘ì—ì„œ ë³´ê¸° ì¢‹ì€ í˜•ì‹)
# --------------------------------------------------------------------------------------
logger = logging.getLogger("konyang.ceph.pipeline")
if not logger.handlers:
    handler = logging.StreamHandler(stream=sys.stdout)
    formatter = logging.Formatter("[%(levelname)s] %(asctime)s | %(message)s")
    handler.setFormatter(formatter)
    logger.addHandler(handler)
logger.setLevel(logging.INFO)

# --------------------------------------------------------------------------------------
# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸ (íŒ¨í‚¤ì§€ ì‹¤í–‰/ì§ì ‘ ì‹¤í–‰ ëª¨ë‘ ì§€ì›)
# --------------------------------------------------------------------------------------
try:
    # íŒ¨í‚¤ì§€ ì»¨í…ìŠ¤íŠ¸ (srcë¥¼ íŒ¨í‚¤ì§€ ë£¨íŠ¸ë¡œ ì¸ì‹)
    from .demo_inference import ImprovedDemoInference as DemoInference
    from .clinical_metrics import compute_all as compute_clinical_metrics
    from .multimodal_classifier import EnhancedDemoClassifier as DemoClassifier
except Exception:
    # ì§ì ‘ ì‹¤í–‰/ìƒëŒ€ ê²½ë¡œ ë¬¸ì œ ëŒ€ì‘
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    from demo_inference import ImprovedDemoInference as DemoInference
    from clinical_metrics import compute_all as compute_clinical_metrics
    from multimodal_classifier import EnhancedDemoClassifier as DemoClassifier


class CephalometricPipeline:
    """
    ì¸¡ë©´ë‘ë¶€ê·œê²©ë°©ì‚¬ì„ ì‚¬ì§„ ë¶„ì„ í†µí•© íŒŒì´í”„ë¼ì¸
    """

    # êµ¬ì„± íŒŒì¼ ê¸°ë³¸ ìƒëŒ€ ê²½ë¡œ
    _DEFAULT_CFG_DIR = Path("data") / "clinical_standards"
    _DEMO_LMK_FILE = "demo_landmarks.json"
    _MEAN_SHAPE_FILE = "mean_shape.json"

    def __init__(
        self,
        demo_mode: bool = True,
        seed: int = 42,
        rule_weight: float = 0.7,
        config_dir: Optional[Union[str, Path]] = None,
    ):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” (ê²½ë¡œ/ì˜ì¡´ì„± ì•ˆì „ ë²„ì „)

        Args:
            demo_mode: ë°ëª¨ ëª¨ë“œ ì‚¬ìš© ì—¬ë¶€
            seed: ë‚œìˆ˜ ì‹œë“œ
            rule_weight: (í–¥í›„ í™•ì¥) ë£° ê¸°ë°˜ ê°€ì¤‘ì¹˜
            config_dir: ì„ìƒ í‘œì¤€ ì„¤ì • íŒŒì¼ ë””ë ‰í† ë¦¬(ë¯¸ì§€ì • ì‹œ ìë™ íƒìƒ‰)
        """
        self.demo_mode = demo_mode
        self.seed = seed
        self.rule_weight = rule_weight

        # ----------------------------- ì„¤ì • ê²½ë¡œ í™•ì • -----------------------------
        # ìš°ì„ ìˆœìœ„: ì¸ì > í™˜ê²½ë³€ìˆ˜(KONYANG_DATA_DIR) > í”„ë¡œì íŠ¸ ë£¨íŠ¸/data/clinical_standards
        if config_dir is not None:
            cfg_dir = Path(config_dir)
        else:
            env_dir = os.environ.get("KONYANG_DATA_DIR")
            if env_dir:
                cfg_dir = Path(env_dir)
            else:
                # integration_pipeline.py ìœ„ì¹˜: src/core/ -> í”„ë¡œì íŠ¸ ë£¨íŠ¸ëŠ” parent.parent
                project_root = Path(__file__).resolve().parent.parent.parent
                cfg_dir = project_root / self._DEFAULT_CFG_DIR

        self.config_dir: Path = cfg_dir
        self.demo_lmk_path = self.config_dir / self._DEMO_LMK_FILE
        self.mean_shape_path = self.config_dir / self._MEAN_SHAPE_FILE

        # ê²½ë¡œ ì¡´ì¬ ì—¬ë¶€ë¥¼ ë¯¸ë¦¬ ë¡œê·¸ë¡œ ì•Œë¦¼(ì—†ì–´ë„ DemoInferenceê°€ ìì²´ fallbackì„ ê°€ì§ˆ ìˆ˜ ìˆìŒ)
        if not self.config_dir.exists():
            logger.warning(f"ì„ìƒ í‘œì¤€ ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.config_dir}")
        if not self.demo_lmk_path.exists():
            logger.warning(f"ë°ëª¨ ëœë“œë§ˆí¬ ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤: {self.demo_lmk_path}")
        if not self.mean_shape_path.exists():
            logger.warning(f"í‰ê·  í˜•íƒœ ì„¤ì •ì´ ì—†ìŠµë‹ˆë‹¤: {self.mean_shape_path}")

        # ----------------------------- ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™” -----------------------------
        if demo_mode:
            try:
                self.inference_engine = DemoInference(
                    demo_config_path=str(self.demo_lmk_path),
                    mean_shape_path=str(self.mean_shape_path),
                    seed=seed,
                )
            except Exception as e:
                # ì¶”ë¡  ì—”ì§„ ì´ˆê¸°í™” ì‹¤íŒ¨ ì‹œ, íŒŒì´í”„ë¼ì¸ì€ ì‚´ì•„ìˆë˜ run()ì—ì„œ ì—ëŸ¬ ë¦¬í„´
                logger.exception("DemoInference ì´ˆê¸°í™” ì‹¤íŒ¨")
                self.inference_engine = None
                self._init_error = ("InferenceInitError", str(e))
            else:
                self._init_error = None

            try:
                self.classifier = DemoClassifier(seed=seed)
            except Exception as e:
                logger.exception("DemoClassifier ì´ˆê¸°í™” ì‹¤íŒ¨")
                self.classifier = None
                self._clf_init_error = ("ClassifierInitError", str(e))
            else:
                self._clf_init_error = None
        else:
            raise NotImplementedError("ì—°êµ¬ ëª¨ë“œëŠ” ì•ˆì „ êµ¬ì—­ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤.")

        # ----------------------------- ì‹¤í–‰ í†µê³„ -----------------------------
        self.stats: Dict[str, Any] = {
            "total_runs": 0,
            "last_run_time": None,
            "average_processing_time": 0.0,
        }

        logger.info(
            f"âœ… CephalometricPipeline ì´ˆê¸°í™” ì™„ë£Œ "
            f"(demo_mode={demo_mode}, cfg='{self.config_dir}')"
        )

    # ----------------------------------------------------------------------------------
    # ë‚´ë¶€ ìœ í‹¸
    # ----------------------------------------------------------------------------------
    @staticmethod
    def _ensure_pil_image(image_input: Union[str, Image.Image]) -> Image.Image:
        """ë¬¸ìì—´ ê²½ë¡œ ë˜ëŠ” PIL.Imageë¥¼ ì¼ê´€ëœ PIL.Imageë¡œ ë³€í™˜."""
        if isinstance(image_input, Image.Image):
            img = image_input
        elif isinstance(image_input, (str, os.PathLike)):
            path = str(image_input)
            if not os.path.exists(path):
                raise FileNotFoundError(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {path}")
            img = Image.open(path)
        else:
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ ì…ë ¥ í˜•ì‹ì…ë‹ˆë‹¤ (str ê²½ë¡œ ë˜ëŠ” PIL.Image í•„ìš”)")

        if img.mode != "RGB":
            img = img.convert("RGB")
        w, h = img.size
        if w < 100 or h < 100:
            raise ValueError(f"ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤: {w}x{h} (ìµœì†Œ 100x100)")
        return img

    def _update_stats(self, processing_time: float) -> None:
        """ì‹¤í–‰ í†µê³„ë¥¼ ì´ë™í‰ê· ìœ¼ë¡œ ê°±ì‹ ."""
        self.stats["total_runs"] += 1
        self.stats["last_run_time"] = processing_time
        if self.stats["total_runs"] == 1:
            self.stats["average_processing_time"] = processing_time
        else:
            alpha = 0.1
            self.stats["average_processing_time"] = (
                alpha * processing_time + (1 - alpha) * self.stats["average_processing_time"]
            )

    @staticmethod
    def _summarize_quality(
        landmarks: Dict[str, Tuple[float, float]],
        clinical_metrics: Dict[str, Any],
        classification: Dict[str, Any],
    ) -> Dict[str, Any]:
        """ê²°ê³¼ í’ˆì§ˆì„ í‰ê°€í•˜ê³  ìš”ì•½."""
        quality_scores: Dict[str, float] = {}
        warnings: List[str] = []

        # 1) ëœë“œë§ˆí¬ í’ˆì§ˆ
        lmk_score = 1.0
        if len(landmarks) < 19:
            lmk_score -= 0.1 * (19 - len(landmarks))
            warnings.append(f"ì¼ë¶€ ëœë“œë§ˆí¬ ëˆ„ë½ ({len(landmarks)}/19)")
        quality_scores["landmarks"] = max(0.0, min(1.0, lmk_score))

        # 2) ì„ìƒ ì§€í‘œ í’ˆì§ˆ(ì •ìƒ/ì´ìƒ ë¹„ìœ¨)
        m_score = 1.0
        abnormal = sum(1 for v in clinical_metrics.values() if v.get("status") != "normal")
        if abnormal >= 3:
            m_score -= 0.2
            warnings.append(f"ë‹¤ìˆ˜ ì§€í‘œ ì´ìƒ ({abnormal}ê°œ)")
        quality_scores["metrics"] = max(0.0, min(1.0, m_score))

        # 3) ë¶„ë¥˜ ì‹ ë¢°ë„
        conf = float(classification.get("confidence", 0.0))
        if conf < 0.7:
            warnings.append(f"ë‚®ì€ ë¶„ë¥˜ ì‹ ë¢°ë„ ({conf * 100:.1f}%)")
        quality_scores["classification"] = max(0.0, min(1.0, conf))

        # 4) ì¢…í•© ì ìˆ˜
        overall = (
            quality_scores["landmarks"] * 0.3
            + quality_scores["metrics"] * 0.3
            + quality_scores["classification"] * 0.4
        )
        overall = max(0.0, min(1.0, overall))

        def rec_text(s: float, warns: List[str]) -> str:
            if s >= 0.9:
                return "ìš°ìˆ˜í•œ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤."
            if s >= 0.7:
                return "ì–‘í˜¸í•œ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤."
            if s >= 0.5:
                return "ë¶„ì„ ê²°ê³¼ë¥¼ ì‹ ì¤‘íˆ ê²€í† í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
            return "ë¶„ì„ ê²°ê³¼ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ í’ˆì§ˆì´ë‚˜ ëœë“œë§ˆí¬ ìœ„ì¹˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."

        return {
            "overall_score": round(overall, 3),
            "component_scores": quality_scores,
            "warnings": warnings,
            "recommendation": rec_text(overall, warnings),
        }

    # ----------------------------------------------------------------------------------
    # ê³µê°œ API
    # ----------------------------------------------------------------------------------
    def preprocess_image(self, image_input: Union[str, Image.Image]) -> Image.Image:
        """ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•˜ì—¬ PIL.Imageë¡œ ë°˜í™˜."""
        return self._ensure_pil_image(image_input)

    def run(
        self,
        image_input: Union[str, Image.Image],
        meta: Optional[Dict[str, Any]] = None,
        anchors: Optional[Dict[str, Tuple[float, float]]] = None,
        run_id: Optional[str] = None,
    ) -> Dict[str, Any]:
        """
        íŒŒì´í”„ë¼ì¸ ì „ì²´ ì‹¤í–‰.

        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬(ì˜¤ë¥˜ ì‹œ 'error' í‚¤ í¬í•¨)
        """
        # ì´ˆê¸°í™” ì—ëŸ¬ê°€ ìˆì—ˆë‹¤ë©´ ì¦‰ì‹œ ë¦¬í„´(ì•±ì€ ê³„ì† ë™ì‘)
        if getattr(self, "_init_error", None):
            etype, emsg = self._init_error
            return {
                "success": False,
                "error": {"type": etype, "message": emsg, "stage": "init"},
                "demo_mode": self.demo_mode,
            }
        if getattr(self, "inference_engine", None) is None or getattr(self, "classifier", None) is None:
            return {
                "success": False,
                "error": {"type": "ComponentMissing", "message": "í•„ìˆ˜ ì»´í¬ë„ŒíŠ¸ê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.", "stage": "init"},
                "demo_mode": self.demo_mode,
            }

        rid = run_id or str(uuid.uuid4())[:8]
        ts = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        meta = meta or {}

        try:
            start = time.perf_counter()

            # 1) ì „ì²˜ë¦¬
            t1 = time.perf_counter()
            image = self.preprocess_image(image_input)
            t2 = time.perf_counter()

            # 2) ëœë“œë§ˆí¬ ì¶”ë¡ 
            landmarks, inference_mode = self.inference_engine.predict_landmarks(image, anchors=anchors)
            t3 = time.perf_counter()

            # 3) ì„ìƒ ì§€í‘œ ê³„ì‚°
            clinical = compute_clinical_metrics(landmarks)
            t4 = time.perf_counter()

            # 4) ë¶„ë¥˜
            cls = self.classifier.predict(clinical, meta)
            t5 = time.perf_counter()

            total_s = t5 - start

            # í’ˆì§ˆ ìš”ì•½
            quality = self._summarize_quality(landmarks, clinical, cls)

            result: Dict[str, Any] = {
                "run_id": rid,
                "timestamp": ts,
                "demo_mode": self.demo_mode,
                "seed": self.seed,
                "image_info": {"size": image.size, "mode": image.mode, "input_type": type(image_input).__name__},
                "meta": meta,
                "anchors_used": anchors is not None,
                "landmarks": {"count": len(landmarks), "inference_mode": inference_mode, "coordinates": landmarks},
                "clinical_metrics": clinical,
                "classification": cls,
                "performance": {
                    "total_time_ms": round(total_s * 1000, 2),
                    "preprocessing_ms": round((t2 - t1) * 1000, 2),
                    "inference_ms": round((t3 - t2) * 1000, 2),
                    "metrics_ms": round((t4 - t3) * 1000, 2),
                    "classification_ms": round((t5 - t4) * 1000, 2),
                },
                "quality": quality,
                "success": True,
            }

            self._update_stats(total_s)
            return result

        except Exception as e:
            logger.exception("íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜ˆì™¸ ë°œìƒ")
            return {
                "run_id": rid,
                "timestamp": ts,
                "demo_mode": self.demo_mode,
                "success": False,
                "error": {
                    "type": type(e).__name__,
                    "message": str(e),
                },
            }

    def get_pipeline_info(self) -> Dict[str, Any]:
        """íŒŒì´í”„ë¼ì¸ ìƒíƒœ/ë²„ì „/ì»´í¬ë„ŒíŠ¸ ë©”íƒ€ë¥¼ ë°˜í™˜."""
        return {
            "pipeline": "CephalometricPipeline",
            "version": "1.0.1",
            "demo_mode": self.demo_mode,
            "seed": self.seed,
            "config_dir": str(self.config_dir),
            "components": {
                "inference_engine": getattr(self.inference_engine, "get_inference_info", lambda: {"name": "unknown"})(),
                "classifier": getattr(self.classifier, "get_classifier_info", lambda: {"name": "unknown"})(),
            },
            "statistics": dict(self.stats),
        }

    def run_batch(
        self,
        image_list: List[Union[str, Image.Image]],
        meta_list: Optional[List[Dict[str, Any]]] = None,
    ) -> List[Dict[str, Any]]:
        """ì—¬ëŸ¬ ì´ë¯¸ì§€ë¥¼ ìˆœì°¨ ì²˜ë¦¬."""
        if meta_list is None:
            meta_list = [{} for _ in image_list]
        if len(meta_list) != len(image_list):
            raise ValueError("ì´ë¯¸ì§€ ê°œìˆ˜ì™€ ë©”íƒ€ë°ì´í„° ê°œìˆ˜ê°€ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤.")

        results: List[Dict[str, Any]] = []
        batch_start = time.perf_counter()

        logger.info(f"ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(image_list)}ê°œ ì´ë¯¸ì§€")
        for i, (img, meta) in enumerate(zip(image_list, meta_list), start=1):
            rid = f"batch_{i:03d}"
            try:
                res = self.run(img, meta=meta, run_id=rid)
                results.append(res)
                if res.get("success"):
                    ms = res["performance"]["total_time_ms"]
                    logger.info(f"   âœ… {i}/{len(image_list)} ì™„ë£Œ ({ms:.1f}ms)")
                else:
                    logger.warning(f"   âš ï¸ {i}/{len(image_list)} ì‹¤íŒ¨: {res.get('error', {}).get('message')}")
            except Exception as e:
                results.append({"run_id": rid, "success": False, "error": {"type": type(e).__name__, "message": str(e)}})
                logger.exception(f"   âŒ {i}/{len(image_list)} ì˜ˆì™¸")

        logger.info(f"ğŸ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {time.perf_counter() - batch_start:.2f}s")
        return results


# --------------------------------------------------------------------------------------
# ë¡œì»¬ í…ŒìŠ¤íŠ¸ìš© ì§„ì…ì 
# --------------------------------------------------------------------------------------
def test_integration_pipeline() -> bool:
    """
    í†µí•© íŒŒì´í”„ë¼ì¸ ê°„ë‹¨ í…ŒìŠ¤íŠ¸ (ë¡œì»¬ ì‹¤í–‰ ì „ìš©)
    """
    print("ğŸ§ª CephalometricPipeline í…ŒìŠ¤íŠ¸")
    print("=" * 60)

    try:
        pipeline = CephalometricPipeline(demo_mode=True, seed=42)
        info = pipeline.get_pipeline_info()
        print("ğŸ“‹ íŒŒì´í”„ë¼ì¸ ì •ë³´:", json.dumps(info, ensure_ascii=False, indent=2))

        # í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ì´ë¯¸ì§€
        img = Image.new("RGB", (800, 600), color="#DDDDDD")

        print("\nğŸš€ ê¸°ë³¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸:")
        meta = {"age": 25, "sex": "F", "patient_id": "TEST001"}
        result = pipeline.run(img, meta=meta)

        if not result.get("success", False):
            print(f"   âŒ ì‹¤í–‰ ì‹¤íŒ¨: {result.get('error')}")
            return False

        print(f"   âœ… ì‹¤í–‰ ì„±ê³µ (ID: {result['run_id']})")
        print(f"   ì´ ì²˜ë¦¬ ì‹œê°„: {result['performance']['total_time_ms']:.1f}ms")
        print(f"   ì¶”ë¡  ëª¨ë“œ: {result['landmarks']['inference_mode']}")
        print(f"   ë¶„ë¥˜ ê²°ê³¼: {result['classification'].get('predicted_label')}")
        print(f"   ì‹ ë¢°ë„: {result['classification'].get('confidence', 0.0) * 100:.1f}%")
        print(f"   í’ˆì§ˆ ì ìˆ˜: {result['quality']['overall_score']:.3f}")

        print("\nğŸ”§ ì•µì»¤ í¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸:")
        anchors = {"Or": (400.0, 200.0), "Po": (300.0, 210.0)}
        res2 = pipeline.run(img, meta=meta, anchors=anchors)
        print(f"   ì•µì»¤ ì‚¬ìš©: {res2.get('anchors_used')} / ëª¨ë“œ: {res2['landmarks']['inference_mode']}")

        print("\nğŸ“Š ì„±ëŠ¥ ì„¸ë¶€:")
        for k, v in result["performance"].items():
            if k.endswith("_ms"):
                print(f"   {k:>18}: {v:>7.1f} ms")

        print("\nğŸ” í’ˆì§ˆ í‰ê°€:")
        q = result["quality"]
        print(f"   ì „ì²´ ì ìˆ˜: {q['overall_score']:.3f}")
        print(f"   ê¶Œì¥ì‚¬í•­: {q['recommendation']}")
        if q["warnings"]:
            print(f"   ê²½ê³ ì‚¬í•­: {', '.join(q['warnings'])}")

        return True

    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback

        traceback.print_exc()
        return False


if __name__ == "__main__":
    test_integration_pipeline()