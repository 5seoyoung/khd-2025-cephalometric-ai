# -*- coding: utf-8 -*-
"""
Integration Pipeline
ì „ì²´ cephalometric ë¶„ì„ ì›Œí¬í”Œë¡œìš°ë¥¼ í†µí•© ì‹¤í–‰í•©ë‹ˆë‹¤.

ì›Œí¬í”Œë¡œìš°:
1. ì´ë¯¸ì§€ ì…ë ¥ ë° ì „ì²˜ë¦¬
2. ëœë“œë§ˆí¬ ì¶”ë¡  (demo_inference)
3. ì„ìƒ ì§€í‘œ ê³„ì‚° (clinical_metrics)
4. ë¶€ì •êµí•© ë¶„ë¥˜ (multimodal_classifier)
5. ê²°ê³¼ í†µí•© ë° ë°˜í™˜
"""

import os
import time
import uuid
from datetime import datetime
from typing import Dict, Any, Optional, Union, Tuple
from PIL import Image
import json

# ë¡œì»¬ ëª¨ë“ˆ ì„í¬íŠ¸
try:
    # íŒ¨í‚¤ì§€ ë‚´ì—ì„œ ì‹¤í–‰ì‹œ
    from .demo_inference import DemoInference
    from .clinical_metrics import compute_all as compute_clinical_metrics
    from .multimodal_classifier import DemoClassifier
except ImportError:
    # ì§ì ‘ ì‹¤í–‰ì‹œ
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.abspath(__file__)))
    
    from demo_inference import DemoInference
    from clinical_metrics import compute_all as compute_clinical_metrics
    from multimodal_classifier import DemoClassifier

class CephalometricPipeline:
    """
    ì¸¡ë©´ë‘ë¶€ê·œê²©ë°©ì‚¬ì„ ì‚¬ì§„ ë¶„ì„ í†µí•© íŒŒì´í”„ë¼ì¸
    """
    
    def __init__(self, 
                 demo_mode: bool = True,
                 seed: int = 42,
                 rule_weight: float = 0.7,
                 config_dir: str = "data/clinical_standards"):
        """
        íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        
        Args:
            demo_mode: ë°ëª¨ ëª¨ë“œ í™œì„±í™” ì—¬ë¶€
            seed: ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ
            rule_weight: ë¶„ë¥˜ê¸°ì˜ ê·œì¹™ ê¸°ë°˜ ê°€ì¤‘ì¹˜
            config_dir: ì„¤ì • íŒŒì¼ ë””ë ‰í† ë¦¬
        """
        self.demo_mode = demo_mode
        self.seed = seed
        self.config_dir = config_dir
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        if demo_mode:
            self.inference_engine = DemoInference(
                demo_config_path=os.path.join(config_dir, "demo_landmarks.json"),
                mean_shape_path=os.path.join(config_dir, "mean_shape.json"),
                seed=seed
            )
            self.classifier = DemoClassifier(seed=seed, rule_weight=rule_weight)
        else:
            raise NotImplementedError("ì—°êµ¬ ëª¨ë“œëŠ” ì•ˆì‹¬ì¡´ì—ì„œë§Œ ì‚¬ìš© ê°€ëŠ¥í•©ë‹ˆë‹¤")
        
        # ì‹¤í–‰ í†µê³„
        self.stats = {
            "total_runs": 0,
            "last_run_time": None,
            "average_processing_time": 0.0
        }
        
        print(f"âœ… CephalometricPipeline ì´ˆê¸°í™” ì™„ë£Œ (demo_mode={demo_mode})")
    
    def preprocess_image(self, image_input: Union[str, Image.Image]) -> Image.Image:
        """
        ì…ë ¥ ì´ë¯¸ì§€ë¥¼ ì „ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            image_input: ì´ë¯¸ì§€ íŒŒì¼ ê²½ë¡œ ë˜ëŠ” PIL Image ê°ì²´
        
        Returns:
            ì „ì²˜ë¦¬ëœ PIL Image
        """
        if isinstance(image_input, str):
            if not os.path.exists(image_input):
                raise FileNotFoundError(f"ì´ë¯¸ì§€ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {image_input}")
            image = Image.open(image_input)
        elif isinstance(image_input, Image.Image):
            image = image_input
        else:
            raise ValueError("ì§€ì›í•˜ì§€ ì•ŠëŠ” ì´ë¯¸ì§€ ì…ë ¥ í˜•ì‹ì…ë‹ˆë‹¤")
        
        # RGB ë³€í™˜ (í•„ìš”ì‹œ)
        if image.mode != "RGB":
            image = image.convert("RGB")
        
        # í¬ê¸° ê²€ì¦
        width, height = image.size
        if width < 100 or height < 100:
            raise ValueError(f"ì´ë¯¸ì§€ê°€ ë„ˆë¬´ ì‘ìŠµë‹ˆë‹¤: {width}x{height}")
        
        return image
    
    def run(self, 
            image_input: Union[str, Image.Image],
            meta: Optional[Dict[str, Any]] = None,
            anchors: Optional[Dict[str, Tuple[float, float]]] = None,
            run_id: Optional[str] = None) -> Dict[str, Any]:
        """
        ì „ì²´ ë¶„ì„ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤.
        
        Args:
            image_input: ì…ë ¥ ì´ë¯¸ì§€
            meta: í™˜ì ë©”íƒ€ë°ì´í„° (ë‚˜ì´, ì„±ë³„ ë“±)
            anchors: ìˆ˜ë™ ì•µì»¤ í¬ì¸íŠ¸ (Or, Po)
            run_id: ì‹¤í–‰ ID (ë¯¸ì œê³µì‹œ ìë™ ìƒì„±)
        
        Returns:
            ë¶„ì„ ê²°ê³¼ ë”•ì…”ë„ˆë¦¬
        """
        # ì‹¤í–‰ ID ë° íƒ€ì´ë° ì„¤ì •
        if run_id is None:
            run_id = str(uuid.uuid4())[:8]
        
        start_time = time.perf_counter()
        timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
        
        if meta is None:
            meta = {}
        
        try:
            # 1ë‹¨ê³„: ì´ë¯¸ì§€ ì „ì²˜ë¦¬
            t1 = time.perf_counter()
            image = self.preprocess_image(image_input)
            t2 = time.perf_counter()
            
            # 2ë‹¨ê³„: ëœë“œë§ˆí¬ ì¶”ë¡ 
            landmarks, inference_mode = self.inference_engine.predict_landmarks(
                image, anchors=anchors
            )
            t3 = time.perf_counter()
            
            # 3ë‹¨ê³„: ì„ìƒ ì§€í‘œ ê³„ì‚°
            clinical_metrics = compute_clinical_metrics(landmarks)
            t4 = time.perf_counter()
            
            # 4ë‹¨ê³„: ë¶€ì •êµí•© ë¶„ë¥˜
            classification_result = self.classifier.predict(clinical_metrics, meta)
            t5 = time.perf_counter()
            
            # ì´ ì²˜ë¦¬ ì‹œê°„
            total_time = t5 - start_time
            
            # ê²°ê³¼ í†µí•©
            result = {
                # ë©”íƒ€ì •ë³´
                "run_id": run_id,
                "timestamp": timestamp,
                "demo_mode": self.demo_mode,
                "seed": self.seed,
                
                # ì…ë ¥ ì •ë³´
                "image_info": {
                    "size": image.size,
                    "mode": image.mode,
                    "input_type": type(image_input).__name__
                },
                "meta": meta,
                "anchors_used": anchors is not None,
                
                # ì²˜ë¦¬ ê²°ê³¼
                "landmarks": {
                    "count": len(landmarks),
                    "inference_mode": inference_mode,
                    "coordinates": landmarks
                },
                "clinical_metrics": clinical_metrics,
                "classification": classification_result,
                
                # ì„±ëŠ¥ ì§€í‘œ
                "performance": {
                    "total_time_ms": round(total_time * 1000, 2),
                    "preprocessing_ms": round((t2 - t1) * 1000, 2),
                    "inference_ms": round((t3 - t2) * 1000, 2),
                    "metrics_ms": round((t4 - t3) * 1000, 2),
                    "classification_ms": round((t5 - t4) * 1000, 2)
                },
                
                # í’ˆì§ˆ ì§€í‘œ
                "quality": self._assess_result_quality(landmarks, clinical_metrics, classification_result)
            }
            
            # í†µê³„ ì—…ë°ì´íŠ¸
            self._update_stats(total_time)
            
            return result
            
        except Exception as e:
            # ì—ëŸ¬ ì •ë³´ í¬í•¨í•œ ê²°ê³¼ ë°˜í™˜
            error_result = {
                "run_id": run_id,
                "timestamp": timestamp,
                "demo_mode": self.demo_mode,
                "error": {
                    "type": type(e).__name__,
                    "message": str(e),
                    "occurred_at": time.perf_counter() - start_time
                },
                "success": False
            }
            
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            return error_result
    
    def _assess_result_quality(self, 
                              landmarks: Dict[str, Tuple[float, float]],
                              clinical_metrics: Dict[str, Any],
                              classification: Dict[str, Any]) -> Dict[str, Any]:
        """
        ê²°ê³¼ í’ˆì§ˆì„ í‰ê°€í•©ë‹ˆë‹¤.
        """
        quality_scores = {}
        warnings = []
        
        # 1. ëœë“œë§ˆí¬ í’ˆì§ˆ í‰ê°€
        landmark_score = 1.0
        if len(landmarks) < 19:
            landmark_score -= 0.1 * (19 - len(landmarks))
            warnings.append(f"ì¼ë¶€ ëœë“œë§ˆí¬ ëˆ„ë½ ({len(landmarks)}/19)")
        
        quality_scores["landmarks"] = max(0.0, landmark_score)
        
        # 2. ì„ìƒ ì§€í‘œ í’ˆì§ˆ í‰ê°€
        metrics_score = 1.0
        abnormal_count = 0
        for metric_name, metric_data in clinical_metrics.items():
            if metric_data["status"] != "normal":
                abnormal_count += 1
        
        if abnormal_count >= 3:
            warnings.append(f"ë‹¤ìˆ˜ ì§€í‘œ ì´ìƒ ({abnormal_count}/4)")
            metrics_score -= 0.2
        
        quality_scores["metrics"] = max(0.0, metrics_score)
        
        # 3. ë¶„ë¥˜ ì‹ ë¢°ë„ í‰ê°€
        classification_confidence = classification["confidence"]
        if classification_confidence < 0.7:
            warnings.append(f"ë‚®ì€ ë¶„ë¥˜ ì‹ ë¢°ë„ ({classification_confidence*100:.1f}%)")
        
        quality_scores["classification"] = classification_confidence
        
        # 4. ì „ì²´ í’ˆì§ˆ ì ìˆ˜
        overall_score = (
            quality_scores["landmarks"] * 0.3 + 
            quality_scores["metrics"] * 0.3 + 
            quality_scores["classification"] * 0.4
        )
        
        return {
            "overall_score": round(overall_score, 3),
            "component_scores": quality_scores,
            "warnings": warnings,
            "recommendation": self._get_quality_recommendation(overall_score, warnings)
        }
    
    def _get_quality_recommendation(self, score: float, warnings: list[str]) -> str:
        """
        í’ˆì§ˆ ì ìˆ˜ì— ë”°ë¥¸ ê¶Œì¥ì‚¬í•­ì„ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        if score >= 0.9:
            return "ìš°ìˆ˜í•œ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤."
        elif score >= 0.7:
            return "ì–‘í˜¸í•œ ë¶„ì„ ê²°ê³¼ì…ë‹ˆë‹¤."
        elif score >= 0.5:
            return "ë¶„ì„ ê²°ê³¼ë¥¼ ì‹ ì¤‘íˆ ê²€í† í•˜ì‹œê¸° ë°”ëë‹ˆë‹¤."
        else:
            return "ë¶„ì„ ê²°ê³¼ì˜ ì‹ ë¢°ë„ê°€ ë‚®ìŠµë‹ˆë‹¤. ì´ë¯¸ì§€ í’ˆì§ˆì´ë‚˜ ëœë“œë§ˆí¬ ìœ„ì¹˜ë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”."
    
    def _update_stats(self, processing_time: float):
        """
        ì‹¤í–‰ í†µê³„ë¥¼ ì—…ë°ì´íŠ¸í•©ë‹ˆë‹¤.
        """
        self.stats["total_runs"] += 1
        self.stats["last_run_time"] = processing_time
        
        # ì´ë™í‰ê· ìœ¼ë¡œ í‰ê·  ì²˜ë¦¬ ì‹œê°„ ì—…ë°ì´íŠ¸
        if self.stats["total_runs"] == 1:
            self.stats["average_processing_time"] = processing_time
        else:
            alpha = 0.1  # ê°€ì¤‘ì¹˜
            self.stats["average_processing_time"] = (
                alpha * processing_time + 
                (1 - alpha) * self.stats["average_processing_time"]
            )
    
    def get_pipeline_info(self) -> Dict[str, Any]:
        """
        íŒŒì´í”„ë¼ì¸ ì •ë³´ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        """
        return {
            "pipeline": "CephalometricPipeline",
            "version": "1.0",
            "demo_mode": self.demo_mode,
            "seed": self.seed,
            "components": {
                "inference_engine": self.inference_engine.get_inference_info(),
                "classifier": self.classifier.get_classifier_info()
            },
            "statistics": self.stats.copy()
        }
    
    def run_batch(self, 
                  image_list: list[Union[str, Image.Image]],
                  meta_list: Optional[list[Dict[str, Any]]] = None) -> list[Dict[str, Any]]:
        """
        ë°°ì¹˜ ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.
        """
        if meta_list is None:
            meta_list = [{}] * len(image_list)
        
        if len(meta_list) != len(image_list):
            raise ValueError("ì´ë¯¸ì§€ì™€ ë©”íƒ€ë°ì´í„° ê°œìˆ˜ê°€ ì¼ì¹˜í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
        
        results = []
        batch_start = time.perf_counter()
        
        print(f"ğŸ”„ ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {len(image_list)}ê°œ ì´ë¯¸ì§€")
        
        for i, (image, meta) in enumerate(zip(image_list, meta_list)):
            try:
                result = self.run(image, meta, run_id=f"batch_{i+1:03d}")
                results.append(result)
                print(f"   âœ… {i+1}/{len(image_list)} ì™„ë£Œ ({result['performance']['total_time_ms']:.1f}ms)")
            except Exception as e:
                error_result = {"run_id": f"batch_{i+1:03d}", "error": str(e), "success": False}
                results.append(error_result)
                print(f"   âŒ {i+1}/{len(image_list)} ì‹¤íŒ¨: {e}")
        
        batch_time = time.perf_counter() - batch_start
        print(f"ğŸ ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {batch_time:.2f}ì´ˆ")
        
        return results

def test_integration_pipeline():
    """
    í†µí•© íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
    """
    print("ğŸ§ª CephalometricPipeline í…ŒìŠ¤íŠ¸")
    print("=" * 60)
    
    try:
        # 1. íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”
        pipeline = CephalometricPipeline(demo_mode=True, seed=42)
        
        # 2. íŒŒì´í”„ë¼ì¸ ì •ë³´ ì¶œë ¥
        info = pipeline.get_pipeline_info()
        print("ğŸ“‹ íŒŒì´í”„ë¼ì¸ ì •ë³´:")
        print(f"   ë²„ì „: {info['version']}")
        print(f"   ë°ëª¨ ëª¨ë“œ: {info['demo_mode']}")
        print(f"   ì‹œë“œ: {info['seed']}")
        
        # 3. í…ŒìŠ¤íŠ¸ìš© ì´ë¯¸ì§€ ìƒì„±
        test_image = Image.new("RGB", (800, 600), color="lightgray")
        
        # 4. ê¸°ë³¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸
        print("\nğŸš€ ê¸°ë³¸ ì‹¤í–‰ í…ŒìŠ¤íŠ¸:")
        meta = {"age": 25, "sex": "F", "patient_id": "TEST001"}
        
        result = pipeline.run(test_image, meta=meta)
        
        if "error" in result:
            print(f"   âŒ ì‹¤í–‰ ì‹¤íŒ¨: {result['error']}")
            return False
        
        print(f"   âœ… ì‹¤í–‰ ì„±ê³µ (ID: {result['run_id']})")
        print(f"   ì´ ì²˜ë¦¬ ì‹œê°„: {result['performance']['total_time_ms']:.1f}ms")
        print(f"   ì¶”ë¡  ëª¨ë“œ: {result['landmarks']['inference_mode']}")
        print(f"   ë¶„ë¥˜ ê²°ê³¼: {result['classification']['predicted_label']}")
        print(f"   ì‹ ë¢°ë„: {result['classification']['confidence']*100:.1f}%")
        print(f"   í’ˆì§ˆ ì ìˆ˜: {result['quality']['overall_score']:.3f}")
        
        # 5. ì•µì»¤ í¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸
        print("\nğŸ”§ ì•µì»¤ í¬ì¸íŠ¸ í…ŒìŠ¤íŠ¸:")
        anchors = {"Or": (400, 200), "Po": (300, 210)}
        result_with_anchors = pipeline.run(test_image, meta=meta, anchors=anchors)
        
        print(f"   ì•µì»¤ ì‚¬ìš©: {result_with_anchors['anchors_used']}")
        print(f"   ì¶”ë¡  ëª¨ë“œ: {result_with_anchors['landmarks']['inference_mode']}")
        
        # 6. ì„±ëŠ¥ ë¹„êµ
        print("\nğŸ“Š ì„±ëŠ¥ ë¶„ì„:")
        perf = result["performance"]
        for stage, time_ms in perf.items():
            if stage.endswith("_ms"):
                stage_name = stage.replace("_ms", "").replace("_", " ").title()
                print(f"   {stage_name}: {time_ms:.1f}ms")
        
        # 7. í’ˆì§ˆ í‰ê°€
        print("\nğŸ” í’ˆì§ˆ í‰ê°€:")
        quality = result["quality"]
        print(f"   ì „ì²´ ì ìˆ˜: {quality['overall_score']:.3f}")
        print(f"   ê¶Œì¥ì‚¬í•­: {quality['recommendation']}")
        if quality["warnings"]:
            print(f"   ê²½ê³ ì‚¬í•­: {', '.join(quality['warnings'])}")
        
        return True
        
    except Exception as e:
        print(f"âŒ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
        import traceback
        traceback.print_exc()
        return False

if __name__ == "__main__":
    test_integration_pipeline()